<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="./includes/a.css">
  <link rel="icon" href="./includes/a.png"> 
  
  <script src='./includes/r_box.js'></script>
  <script src='./includes/r_matrix.js'></script>
  
  <!-- the wonderful katex ! -->
  <link rel="stylesheet" href="./includes/katex/katex.min.css">
  <script src="./includes/katex/katex.min.js"></script>
  <script src="./includes/a.js"></script>

  <title> a3 q6 &middot; </title>

</head>


<div class='section border margin padding'>
  <h4> a3 q6 </h4>
  <p> when selecting the tuning parameter for the LASSO, there is a variance-bias trade-off </p>
  <p> the objective of this exercise is to examine and discuss this trade-off </p>
</div>

<div class='section border margin padding'>

<h4> the model </h4>

<p> there are 30 regressors <span class='math'>X</span>, partitioned into 2 groups : <span class='math'>X_1</span> and <span class='math'>X_2</span>. the ones that matter are <span class='math'>X_1</span>, and <span class='math'>\text{count}(X_1)=10</span>. the ones that dont matter are <span class='math'>X_2</span>, and <span class='math'>\text{count}(X_2)=20</span> </p>

<p> the correlation between all regressors in <span class='math'>X</span> will be either of the following <span class='math'> \rho = 0.1, 0.7 </span></p>
<p> the error, <span class='math'>e \backsim N(0, \sigma^2) </span> is homoskedastic where <span class='math'> \sigma^2 = 1, 10 </span> </p> 
<p> the true model is <span class='math'>Y = X_1 \beta_1 + e </span> where <span class='math'>\beta_{1i} \backsim \text{unif}(0.5, 1)</span></p>
</div>

<div class='section border margin padding'>

<h4> compare lasso vs ols : approach </h4>

<ul>
 <li><p> there are 4 sets. a set defines the specs, <span class='math'> \rho </span> and <span class='math'> \sigma^2 </p>
 <li><p> each set has 1000 reps </p>
 <li><p> each rep we generate <span class='math'>N=80</span> obs, partitioned into 2 groups : <span class='math'>N_1=70</span> training observations, and <span class='math'>N_2 = 10</span> testing observations </p>
 <li><p> train lasso on <span class='math'>N_1</span> using <span class='math'>X_1</span> and <span class='math'>X_2</span> to generate a model, then use the model to make predictions on <span class='math'>N_2</span> </p>
 <li><p> perform ols on <span class='math'>N_1</span> using <span class='math'>X_1</span> to generate another model, and test on <span class='math'>N_2</span> </p>
</ul>

<p> we're looking to compare the following : </p>
<ul>
<li><p> bias, </p>
<li><p> variance, </p>
<li><p> MSE </p>
</ul>
<p> compare them to the OLS model for the last 10 obs in each small dataset in each scenario </p>
<p> discuss the consequences of using a misspecified linear model chosen by the LASSO as the error variance, correlation and magnitude of the coefficients dropped by the LASSO varies as it selects different number of covariates </p>

</div>

<div class='section border margin padding'>

<h4> variance-bias trade-off </h4>

<p> MSE = (variance) + (bias)^2 </p>
<p><span class='math'> E[X^2] = var(X) + (E[X])^2 </span></p>
<p><span class='math'> \text{MSE}(\hat{\theta}) = \text{E}[(\hat{\theta} - \theta)^2] </span></p>
<p><span class='math'> \text{MSE}(\hat{\theta}) = var(\hat{\theta}) + [\text{bias}(\hat{\theta}, \theta)]^2 </span></p>

<p> the high level is that, if we only care about not being biased, we can stick with OLS </p>
<p> we might care about that ideologically, but if we unmarry our analysis from this idea, we can explore biased solutions, with lower variance, with the objective being increasing the probability that the true value falls within confidence intervals </p>
<p> by moving away from OLS, MSE goes down as variance goes down, but to offset that, abs(bias) goes up </p>
<p> if we limit ourselves to unbiased estimators, then shrinking MSE is the same as shrinking the variance </p>

<p> part of what makes the bias-variance thing so hot is that we want to develop a model that picks out what appears frequently, without overfitting the data. the point is to have sthing that we can generalize. </p>

</div>


<div class='section border margin padding'>
<h4> case 1 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,1) </span></p>
<li><p><span class='math'> \rho = 0.1 </span></p>
</ul>

<img src='./a3_desc/a3_q6_sum(abs(beta))_v_log(lambda)_case_1.png' />

</div>

<div class='section border margin padding'>
<h4> case 2 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,1) </span></p>
<li><p><span class='math'> \rho = 0.7 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<h4> case 3 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,10) </span></p>
<li><p><span class='math'> \rho = 0.1 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<h4> case 4 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,10) </span></p>
<li><p><span class='math'> \rho = 0.7 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<pre>
<span class='comment'># to do a lasso</span>
do_lasso_and_lasso_related_activities = function(lambda_vector, train_df) {
  
  # specify and train lasso regression model
  ctrl_specs = trainControl(method="cv", # cross-validation
                            number=10,     # number of folds
                            savePredictions = "all")
  
  # specify lasso regression model using the training data # 52/78
  model = train(y ~ .,
                data=train_df,
                preProcess=c("center","scale"),
                method="glmnet",
                metric="RMSE",
                tuneGrid=expand.grid(alpha=1, lambda=lambda_vector),
                trControl=ctrl_specs,
                na.action=na.omit) # faire en sorte que blah blah blah
  
  return(model)
}</pre>
</div>


<div class='section border margin padding'>
<pre>
<span class='comment'># returns a df</span>
library('MASS')

# returns a df
generate_the_data = function(rho, sigma2, coefficients) {

  n = 80
  k = 30

  mu_vector = rep(0, times = k)
  sigma_vector = rep(c(1,rep(rho, times = k)), length=k*k)
  sigma_matrix = matrix(sigma_vector, nrow=k) # makes the variance-covariance matrix
  
  matrix_data = mvrnorm(n = n,
                 mu = mu_vector,
                 Sigma = sigma_matrix,
                 empirical=TRUE)
  
  e = rnorm(n, 0, sqrt(sigma2))
  
  df_x = as.data.frame(matrix_data)

  y = coefficients[1] * df_x$V1 +
      coefficients[2] * df_x$V2 +
      coefficients[3] * df_x$V3 +
      coefficients[4] * df_x$V4 +
      coefficients[5] * df_x$V5 +
      coefficients[6] * df_x$V6 +
      coefficients[7] * df_x$V7 +
      coefficients[8] * df_x$V8 +
      coefficients[9] * df_x$V9 +
      coefficients[10] * df_x$V10 +
      e
  
  df = data.frame(y, df_x)
  return(df)
}</pre>
</div>
