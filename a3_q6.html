<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="./includes/a.css">
  <link rel="icon" href="./includes/a.png"> 
  
  <script src='./includes/r_box.js'></script>
  <script src='./includes/r_matrix.js'></script>
  
  <!-- the wonderful katex ! -->
  <link rel="stylesheet" href="./includes/katex/katex.min.css">
  <script src="./includes/katex/katex.min.js"></script>
  <script src="./includes/a.js"></script>

  <title> a3 q6 &middot; </title>

</head>


<div class='section border margin padding'>
  <h4> a3 q6 </h4>
  <p> when selecting the tuning parameter for the LASSO, there is a variance-bias trade-off. the objective of this exercise is to examine and discuss this trade-off </p>
</div>

<div class='section border margin padding'>

<h4> the model </h4>

<p> there are 30 regressors <span class='math'>X</span>, partitioned into 2 groups : <span class='math'>X_1</span> and <span class='math'>X_2</span>. the ones that matter are <span class='math'>X_1</span>, and <span class='math'>\text{count}(X_1)=10</span>. the ones that dont matter are <span class='math'>X_2</span>, and <span class='math'>\text{count}(X_2)=20</span> </p>

<p> the correlation between all regressors in <span class='math'>X</span> will be either of the following <span class='math'> \rho = 0.1, 0.7 </span></p>
<p> the error, <span class='math'>e \backsim N(0, \sigma^2) </span> is homoskedastic where <span class='math'> \sigma^2 = 1, 10 </span> </p> 
<p> the true model is <span class='math'>Y = X_1 \beta_1 + e </span> where <span class='math'>\beta_{1i} \backsim \text{unif}(0.5, 1)</span></p>
</div>

<div class='section border margin padding'>

<h4> compare lasso vs ols : approach </h4>

<ul>
 <li><p> there are 4 sets. a set defines the specs, <span class='math'> \rho </span> and <span class='math'> \sigma^2 </p>
 <li><p> each set has 1000 reps </p>
 <li><p> each rep we generate <span class='math'>N=80</span> obs, partitioned into 2 groups : <span class='math'>N_1=70</span> training observations, and <span class='math'>N_2 = 10</span> testing observations </p>
 <li><p> train lasso on <span class='math'>N_1</span> using <span class='math'>X_1</span> and <span class='math'>X_2</span> to generate a model, then use the model to make predictions on <span class='math'>N_2</span> </p>
 <li><p> perform ols on <span class='math'>N_1</span> using <span class='math'>X_1</span> to generate another model, and test on <span class='math'>N_2</span> </p>
</ul>

<p> we're looking to compare the following : </p>
<ul>
<li><p> bias, </p>
<li><p> variance, </p>
<li><p> MSE </p>
</ul>
<p> compare them to the OLS model for the last 10 obs in each small dataset in each scenario </p>
<p> discuss the consequences of using a misspecified linear model chosen by the LASSO as the error variance, correlation and magnitude of the coefficients dropped by the LASSO varies as it selects different number of covariates </p>

</div>

<div class='section border margin padding'>

<h4> variance-bias trade-off </h4>
<p> we can start in very general terms, with <span class='math'> E[X^2] = \text{var}(X) + (E[X])^2 </span>, and let <span class='math'>X = \theta  - \hat{\theta} </span></p>

<p><span class='math'> \begin{aligned} E[(\theta  - \hat{\theta})^2] &= var(\theta  - \hat{\theta}) + (E[\theta  - \hat{\theta}])^2 \\ \text{MSE}(\hat{\theta}) &= \text{var}(\hat{\theta}) + (E[\theta  - \hat{\theta}])^2 &\text{ where } \theta \text{ is a constant} \\ &= \text{var}(\hat{\theta}) + \big [ \text{BIAS}(\hat{\theta}) \big ] ^2 &\text{ } \end{aligned} </span></p>

<p> LASSO considers biased estimates <span class='math'>\hat{\theta}</span> with the objective of minimizing <span class='math'>\text{MSE}(\hat{\theta})</span>. if we consider only unbiased estimates, we can stick with OLS, which minimizes MSE contingent on that unbiasedness. that is the same as finding the unbiased estimator with the smallest variance. but a variance lower than that, despite a bias, may increase the predictive ability of the model. LASSO helps us export this trade-off </p>

</div>


<div class='section border margin padding'>
<h4> case 1 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,1) </span></p>
<li><p><span class='math'> \rho = 0.1 </span></p>
</ul>


<img src='./a3_desc/a3_q6_sum(abs(beta))_histogram_case_1.png' />

<img src='./a3_desc/a3_q6_sum(abs(beta))_v_log(lambda)_case_1.png' />

</div>

<div class='section border margin padding'>
<h4> case 2 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,1) </span></p>
<li><p><span class='math'> \rho = 0.7 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<h4> case 3 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,10) </span></p>
<li><p><span class='math'> \rho = 0.1 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<h4> case 4 </h4>
<ul>
<li><p><span class='math'> e_i \backsim N(0,10) </span></p>
<li><p><span class='math'> \rho = 0.7 </span></p>
</ul>
</div>

<div class='section border margin padding'>
<pre>
<span class='comment'># to do a lasso</span>
do_lasso_and_lasso_related_activities = function(lambda_vector, train_df) {
  
  # specify and train lasso regression model
  ctrl_specs = trainControl(method="cv", # cross-validation
                            number=10,     # number of folds
                            savePredictions = "all")
  
  # specify lasso regression model using the training data # 52/78
  model = train(y ~ .,
                data=train_df,
                preProcess=c("center","scale"),
                method="glmnet",
                metric="RMSE",
                tuneGrid=expand.grid(alpha=1, lambda=lambda_vector),
                trControl=ctrl_specs,
                na.action=na.omit) # faire en sorte que blah blah blah
  
  return(model)
}</pre>
</div>


<div class='section border margin padding'>
<pre>
<span class='comment'># returns a df</span>
library('MASS')

# returns a df
generate_the_data = function(rho, sigma2, coefficients) {

  n = 80
  k = 30

  mu_vector = rep(0, times = k)
  sigma_vector = rep(c(1,rep(rho, times = k)), length=k*k)
  sigma_matrix = matrix(sigma_vector, nrow=k) # makes the variance-covariance matrix
  
  matrix_data = mvrnorm(n = n,
                 mu = mu_vector,
                 Sigma = sigma_matrix,
                 empirical=TRUE)
  
  e = rnorm(n, 0, sqrt(sigma2))
  
  df_x = as.data.frame(matrix_data)

  y = coefficients[1] * df_x$V1 +
      coefficients[2] * df_x$V2 +
      coefficients[3] * df_x$V3 +
      coefficients[4] * df_x$V4 +
      coefficients[5] * df_x$V5 +
      coefficients[6] * df_x$V6 +
      coefficients[7] * df_x$V7 +
      coefficients[8] * df_x$V8 +
      coefficients[9] * df_x$V9 +
      coefficients[10] * df_x$V10 +
      e
  
  df = data.frame(y, df_x)
  return(df)
}</pre>
</div>
