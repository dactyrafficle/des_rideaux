<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="./includes/a.css">
  <link rel="icon" href="./includes/a.png"> 
  
  <script src='./includes/r_box.js'></script>
  <script src='./includes/r_matrix.js'></script>
  
  <!-- the wonderful katex ! -->
  <link rel="stylesheet" href="./includes/katex/katex.min.css">
  <script src="./includes/katex/katex.min.js"></script>
  <script src="./includes/a.js"></script>

  <title> a3 q1 &middot; </title>

</head>



<div class='section border margin padding'>
<h4> A4 Q1 PART-A : 2SLS vs IV </h4>
<pre>
<span class='comment'># PART A : 2SLS vs IV </span>

library(AER)

y = df$log_gdp
x = df$prot_score
z = df$log_mort

<span class='comment'># voici je fais du 2SLS </span>

<span class='comment'># first stage</span>
<span class='comment'># returns F-stat = 23.82 &gt; 10 on q = 1 restriction, and df = (n - k - 1) = (64 - 1 - 1) = 62</span>
model_s1 = lm(x ~ z)
coeftest(model_s1, vcov = vcovHC, type = "HC1")

<span class='comment'># first stage estimates</span>
x_hat = model_s1$fitted.values

<span class='comment'># second stage </span>
<span class='comment'># returns a coefficient 0.91708 (0.11757); t = 7.8003 </span>
model_s2 = lm(y ~ x_hat)
coeftest(model_s2, vcov = vcovHC, type = "HC1")</pre>
</div>

<div class='section border margin padding'>
<pre>
<span class='comment'># et l'IV qui vient d'une boite </span>
<span class='comment'># returns a coefficient 0.91708 (0.16911); t = 5.4299  </span>
model_iv = ivreg(y ~ x | z)
coeftest(model_iv, vcov = vcovHC, type = "HC1")</pre>
</div>


<div class='section border margin padding'>
<h4> A4 Q1 PART-B : COMPARE TO OLS </h4>
<pre>
<span class='comment'># PART B : OLS</span>

<span class='comment'># returns a coefficient 0.522107 (0.049923) t = 10.458 </span>
model_ols = lm(y ~ x)
coeftest(model_ols, vcov = vcovHC, type = "HC1")
</pre>
</div>

<div class='section border margin padding'>
<h4> A4 Q1 PART-C : THE TEST OF HAUSMAN </h4>

<p> in the present context, the test of Hausman is a test to provide evidence for the exogeneity of <span class='math'>x</span> by comparing the estimate of <span class='math'>\beta_1</span> under OLS to an estimate under 2SLS </p>

<p> if <span class='math'>x</span> is really exogenous, then the estimate of <span class='math'>\beta_1</span> from OLS should be similar to the estimate from 2SLS </p>
<p> how similar? </p>
<p> it is the Hausman which demonstrated the following expression follows a chi-squared distribution </p>

<p><span class='math'> H = \dfrac{ (\beta_1^{OLS}-\beta_1^{2SLS})^T (\beta_1^{OLS}-\beta_1^{2SLS})}{ ( \text{var}(\beta_1^{OLS}) - \text{var} (\beta_1^{2SLS}) )} </span></p>

<p><span class='math'> H \backsim \chi(df) </span>, where the number of degrees of freedom equals the number of explanatory variables in the model </p>

<p> but <span class='inline-code'>it relies on the assumption of homoskedasticity</span>, since it needs the denominator of the test stat to be a PSD matrix </p>

<p> but homoskedasticity is such a strong strong assumption </p>
<p> if there is any heterogeneity, the test above is no longer reliable </p>
<p> so there are lots of ways it can fail </p>

<p> the best you can do is try showing that x is exogenous, given the assumption of homoskedasticity, in which case the test of Hausman does give a measure of the similarity between OLS and IV </p>

<pre>

<span class='comment'># here is some code for STATA </span>
ivreg y (x = z)
predict ivresid,residuals
est store ivreg
reg y x
hausman ivreg ., constant sigmamore df(1)
hausman ivreg ., sigmamore df(1)
hausman ivreg ., df(1)

hausman ivreg .,constant         chi2(2) = 8.29, Prob>chi2 =      0.0158
hausman ivreg .                  chi2(1) = 8.29, Prob>chi2 =      0.0040

</pre>

<table class='mytables'>
<tr><td></td><td>ols</td><td>tsls</td></tr>
<tr><td>coefficient </td><td> 0.522107</td><td>0.9170797</td><td>0.156003434 </td></tr>
<tr><td>SE </td><td> 0.061185	</td><td> 0.1501859	 </td></tr>
<tr><td>var </td><td> 0.003743604 </td><td> 0.022555805	</td><td> 0.0188122 </td></tr>

<tr><td colspan=3> chi2 : </td><td colspan=1>8.292673423</td></tr>
</table>

</div>

<!-- opening L0 -->
<div class='section border margin padding'>
  <h4> A4 Q1 PART-D : THE ESTIMATE OF WALD </h4>

<pre>
d. log_gdp on log_mort, then prot_score on log_mort. explain how we get the iv estimate. wald estimate
   this really does look like an application of FWL

the wald estimator, 12.11 of hansen p343
y on z returns : -.5697981
x on z returns :  -.621318
 
</pre>

<span class='math'> \hat{\beta} = \dfrac{-0.5697981}{-0.621318} =  0.917079660 </span></p>

<p> Hansen describes it as the change in Y due to Z over the change in X due to Z </p>

</div><!-- closing L0 -->

<!-- opening L0 -->
<div class='section border margin padding'>
  <h4> A4 Q1 PART-E : THE MATH, AND A COMMENT ON WEAK INSTRUMENTS </h4>


<pre>
e. a-d, do the math. write why weak instruments can be a problem.

write up each of the regression equations and explain how the coefficients are related

please provide an explanation of why weak instruments are problematic from these equations.

</pre>

<p> if the instruments are weak, the tsls estimator is no longer asymptotically normal </p>
<p> so we don't really have any basis for doing the usual statistical inference, as those techniques do not extend to the case when z is weakly relevant </p>

<p> in a nutshell, if the instrument is weak, it can be badly biased towards the ols estimator </p>
<p> and TSLS is no longer reliable </p>

<h4> how weak ? </h4>
<p> just like in the test of Hausman, where he showed H was approx chi-sq. there, under homoskedasticity, we have some objective measure </p>
<p> here, the case is more complex </p>


<h4> a look at the standard error </h4>
<p> in this case, we have one endogenous regressor, one instrument, and no exogenous regressors </p>
<p><span class='math'> \hat{\beta_1}^{TSLS} = \dfrac{s_{zy}}{s_{zx}} \xrightarrow{p} \dfrac{cov(z,y)}{cov(z,x)} = \beta_1 </span></p>

<p> as the instrument becomes less relevant, s_{zx} \xrightarrow{p} cov(z,x) = 0 </p>

<h4> the denominator goes to 0 </h4>

<p> clearly, the argument that b_tsls is consistent breaks down if z is not relevant </p>

<p> when z is irrelevant, the large sample distribution of b_tsls is not normal, but rather a distribution of a ratio of 2 normal random variables </p>

<p> the sample average is consistent, so <span class='math'> \bar{x} \xrightarrow{p} \mu_x </span>, and so on </p>
<p><span class='math'> s_{zx} = \dfrac{1}{n} \cdot (z-\mu_z) (x-\mu_x) = \dfrac{1}{n} \cdot r = \bar{r} </span></p>

<p><span class='math'> var{(\bar{r})} = \frac{var(r)}{n} </span></p>

<p><span class='math'> \hat{\beta_1}^{TSLS} = \beta_1 + \cfrac{\frac{1}{n} \cdot (z-\mu_z)e }{ \frac{1}{n} \cdot (z-\mu_z)(x-\mu_x)} = \beta_1 + \cfrac{\frac{1}{n} \cdot q }{ \frac{1}{n} \cdot r} = \beta_1 + \cfrac{ \bar{q} }{ \bar{r} } </span></p>

<p><span class='math'> var(\bar{r}) = var((z-\mu_z)(x-\mu_x)) </span></p>

<p><span class='math'> \hat{\beta_1}^{TSLS} = \beta_1 + \cfrac{ \bar{q} }{ \bar{r} } = \beta_1 + \dfrac{var(\bar{r})}{var(\bar{r})} \cdot \dfrac{var({\bar{q}})}{var(\bar{q})} \cdot \cfrac{ \bar{q} }{ \bar{r} } = \beta_1 + \cfrac{ \frac{1}{n} \cdot var({q})}{ \frac{1}{n} \cdot var({r})} \cdot \cfrac{ \bar{q} / var({\bar{q}}) }{ \bar{r} / var({\bar{r}}) } = \beta_1 + \cfrac{ var({q})}{ var({r})} \cdot \cfrac{ \bar{q} / var({\bar{q}}) }{ \bar{r} / var({\bar{r}}) } </span></p>

<p><span class='math'> \hat{\beta_1}^{TSLS} \xrightarrow{p} \beta_1 + \cfrac{ \sigma_{q}}{ \sigma_{r}} \cdot \cfrac{ \bar{q} / \sigma_{\bar{q}} }{ \bar{r} / \sigma_{\bar{r}} } </span></p>

<p><span class='math'> \bar{r} / \sigma_{\bar{r}} \xrightarrow{p} N(0,1) </span></p>
<p><span class='math'> \bar{q} / \sigma_{\bar{q}} \xrightarrow{p} N(0,1) </span></p>

<p> and since x and e are related, these 2 terms are related, and that makes the distribution complex </p>

<p> when z is relevant, the numerator in the bias term <span class='math'> \dfrac{cov(z,e)}{cov(z,x)}</span> goes to 0. if z is not relevant, the denominator goes to 0 too. that means both numerator and denominator go to zero. that makes life hard for us.</p>

<p> if z is exogenous (numerator to 0) and relevant (denominator approaches some number), then as n becomes large, the numerator gets smaller and smaller, the denominator gets closer and closer to a given number. </p>

<p> how relevant must the instrument be for the normal distribution to provide a good approx in practice >

<h4> enter the first-stage F-test </h4>

</div>

